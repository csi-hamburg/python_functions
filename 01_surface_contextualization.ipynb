{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface contextualization\n",
    "\n",
    "> Module for surface contextualization of cortical indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp surface_contextualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_surface(\n",
    "    surface:str, # Surface to plot on (default: fslr32k). Valid choices are “fslr32k”, “fsaverage”, “fsaverage3”, “fsaverage4”, “fsaverage5”, “fsaverage6”, “civet41k”, “civet164k”.\n",
    "    values, # numpy array of values to plot (has to match the number of vertices in the surface)\n",
    "    label_text:str, # text to label the plot\n",
    "    color_range:tuple=None, # color range to use\n",
    "    cmap=\"Blues\" # color map to use\n",
    "    )->object: # Plot \n",
    "    \"Plots metric values on surface\"\n",
    "    \n",
    "    # Plot cortical surfaces with values as the data, label_text as\n",
    "    # the labels, and color_range as the limits of the color bar.\n",
    "    from brainstat.datasets import fetch_mask, fetch_template_surface\n",
    "\n",
    "    # Load behavioral markers\n",
    "    pial_left, pial_right = fetch_template_surface(surface, join=False)\n",
    "    pial_combined = fetch_template_surface(surface, join=True)\n",
    "    mask = fetch_mask(surface)\n",
    "\n",
    "    \n",
    "    from brainspace.plotting import plot_hemispheres\n",
    "    import numpy as np\n",
    "\n",
    "    if not color_range:\n",
    "        color_range = (np.nanmin(values), np.nanmax(values))\n",
    "\n",
    "    return plot_hemispheres(\n",
    "        pial_left,\n",
    "        pial_right,\n",
    "        values,\n",
    "        color_bar=True,\n",
    "        color_range=color_range,\n",
    "        label_text=[label_text],\n",
    "        cmap=cmap,\n",
    "        embed_nb=True,\n",
    "        size=(1400, 200),\n",
    "        zoom=1.45,\n",
    "        nan_color=(0.5, 0.5, 0.5, 1),\n",
    "        cb__labelTextProperty={\"fontSize\": 12},\n",
    "        interactive=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def surface_to_schaefer(\n",
    "    array, # array with surface values, has to match vertex count\n",
    "    surface:str, # surface to parcellate; valid choices: ('fslr32k')\n",
    "    atlas_resolution:str, # atlas resolution; valid choices: ('400x7')\n",
    "):\n",
    "    \"Harnesses neuromaps to schaefer-parcellate metrics on surface\"\n",
    "\n",
    "    from neuromaps.parcellate import Parcellater\n",
    "    from netneurotools import datasets as nntdata\n",
    "    from neuromaps.images import dlabel_to_gifti\n",
    "\n",
    "    parc_space_dict = {\n",
    "        'fslr32k':'fsLR'\n",
    "    }\n",
    "\n",
    "    atlas_dict = {\n",
    "        \"400x7\":'400Parcels7Networks',\n",
    "        \"200x7\":'200Parcels7Networks',\n",
    "        \"100x7\":'100Parcels7Networks',\n",
    "        \"400x17\":'400Parcels17Networks',\n",
    "        \"200x17\":'200Parcels17Networks',\n",
    "        \"100x17\":'100Parcels17Networks',\n",
    "    }\n",
    "\n",
    "    schaefer = nntdata.fetch_schaefer2018(surface)[atlas_dict[atlas_resolution]]\n",
    "    parc = Parcellater(dlabel_to_gifti(schaefer), parc_space_dict[surface])\n",
    "\n",
    "    return parc.fit_transform(array.squeeze(), parc_space_dict[surface])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def perform_spins(\n",
    "    array, # array with Schaefer-parcellated metric\n",
    "    reference_array, # reference array with Schaefer-parcellated metric\n",
    "    atlas_resolution:str, # atlas resolution; valid choices: (\"100\",\"200\",'400')\n",
    "    ):\n",
    "    \"Perform spin permutations on Schaefer-parcellated data; returns spearman correlation, p-value and permuted correlations\"\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.stats import spearmanr\n",
    "    from enigmatoolbox.permutation_testing import spin_test\n",
    "\n",
    "    # Perform spatial correlations\n",
    "    fin_idx = np.isfinite(array) & np.isfinite(reference_array)\n",
    "    r = spearmanr(array[fin_idx], reference_array[fin_idx])[0]\n",
    "    p, d = spin_test(array, reference_array, surface_name='fsa5', parcellation_name=f'schaefer_{atlas_resolution}',\n",
    "                                type='spearman', n_rot=1000, null_dist=True)\n",
    "\n",
    "\n",
    "    return r, p, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_null_distributions(\n",
    "    r:float, # correlation between metric and reference\n",
    "    p:float, # p-value\n",
    "    d, # array with permuted correlations\n",
    "    xlabel:str, # x axis label\n",
    "    color:str, # color to use for the plot (hex code, rgb, or named color)\n",
    "    ):\n",
    "    \"Plots null distribution of correlations\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "\n",
    "    # Plot null distributions\n",
    "\n",
    "    axs.hist(d, bins=50, density=True, color=color, edgecolor='white', lw=0.5)\n",
    "\n",
    "    axs.axvline(r, lw=1.5, ls='--', color='k', dashes=(2, 3),\n",
    "\n",
    "                label=f'$r_{{sp}}$={r:.2f}' + f'\\n$p_{{spin}}$={p:.3f}')\n",
    "\n",
    "    axs.set_xlabel(f'Null correlations \\n ({xlabel})')\n",
    "\n",
    "    axs.set_ylabel('Density')\n",
    "\n",
    "    axs.spines['top'].set_visible(False)\n",
    "\n",
    "    axs.spines['right'].set_visible(False)\n",
    "\n",
    "    axs.legend(loc=1, frameon=False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def weighted_degree_centrality(\n",
    "    mat, # adjacency matrix\n",
    "    rank:bool=False, # whether to return the rank of the centrality measure\n",
    "):\n",
    "    \"Compute weighted degree centrality measures from the connectivity data\"\n",
    "    import numpy as np\n",
    "    \n",
    "    dc = np.sum(mat, axis=0)\n",
    "\n",
    "    if rank==True:\n",
    "        return np.argsort(np.argsort(dc * -1))\n",
    "    \n",
    "    return dc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def yeo_participation_coefficient(\n",
    "    mat_df, # adjacency matrix as pandas dataframe with row and column index corresponding with Schaefer-parcellated region labels\n",
    "    rank:bool=False, # whether to return the rank of participation coefficient\n",
    "):\n",
    "    \"Computes participation coefficient of node with regard to Yeo networks\"\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    yeo_networks = [\"Vis\",\"SomMot\",\"DorsAttn\", \"VentAttn\", \"Limbic\", \"Cont\", \"Default\"]\n",
    "\n",
    "    mat_degree = mat_df.sum(axis=0)\n",
    "    rois = mat_df.columns\n",
    "\n",
    "    PC_array = []\n",
    "\n",
    "    for roi in rois:\n",
    "\n",
    "        network_array = []\n",
    "        for network in yeo_networks:\n",
    "            \n",
    "            roi_degree = mat_degree[roi]\n",
    "            \n",
    "            network_rois = [roi for roi in rois if network in roi]\n",
    "            roi_to_network_df = mat_df.loc[roi,network_rois]\n",
    "\n",
    "            roi_to_network_connectivity = roi_to_network_df.sum()\n",
    "\n",
    "            network_array = np.append(network_array, (roi_to_network_connectivity / roi_degree) ** 2)\n",
    "\n",
    "        PC_array = np.append(PC_array, (1 - network_array.sum()))\n",
    "\n",
    "    if rank==True:\n",
    "        return np.argsort(np.argsort(PC_array))\n",
    "\n",
    "    return PC_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def neighborhood_abnormality(\n",
    "    adjacency, # adjacency matrix \n",
    "    metric, # nodewise metric matching node count of adjacency matrix \n",
    "    atlas_labels # atlas labels\n",
    "    ):\n",
    "    \"Computes neighborhood abnormality index\"\n",
    "    import numpy as np\n",
    "\n",
    "    nghbr_dict = {}\n",
    "\n",
    "    for i in range(adjacency.shape[0]):\n",
    "\n",
    "        mask_connected_nodes = adjacency[i] != 0\n",
    "        n_connected_nodes = len(adjacency[i][mask_connected_nodes])\n",
    "\n",
    "        weighted_measure_list = []\n",
    "\n",
    "        for j in range(adjacency.shape[1]):\n",
    "\n",
    "            if adjacency[i,j] != 0: weighted_measure_list.append(metric[j] * adjacency[i,j])\n",
    "\n",
    "        weighted_measure_array = np.array(weighted_measure_list)\n",
    "        sum_weighted_measure = sum(weighted_measure_array[~np.isnan(weighted_measure_array)])\n",
    "\n",
    "        nghbr_dict[atlas_labels[i]] = sum_weighted_measure / n_connected_nodes\n",
    "        \n",
    "    return np.array(list(nghbr_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def connectivity_gradients(\n",
    "    mat, # schaefer adjacency matrix\n",
    "    gradient_nr, # gradient number to return\n",
    "    atlas_resolution:str, # atlas resolution; valid choices: (\"100\",\"200\",'400')\n",
    "    ):\n",
    "\n",
    "    import numpy as np\n",
    "    from brainspace.datasets import  load_parcellation\n",
    "    from brainspace.gradient import GradientMaps\n",
    "    from brainspace.utils.parcellation import map_to_labels\n",
    "\n",
    "    # Ask for 10 gradients (default)\n",
    "    gm = GradientMaps(n_components=10, random_state=0)\n",
    "    gm.fit(mat)\n",
    "\n",
    "    # Load Schaefer parcellation mapping to conte69\n",
    "    labeling = load_parcellation('schaefer', scale=atlas_resolution, join=True)\n",
    "    mask = labeling != 0\n",
    "\n",
    "    grad = [None] * 2\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        # map the gradient to the parcels\n",
    "        grad[i] = map_to_labels(gm.gradients_[:, i], labeling, mask=mask, fill=np.nan)\n",
    "\n",
    "    grad_schaefer = surface_to_schaefer(grad[gradient_nr], surface=\"fslr32k\", atlas_resolution=f\"{atlas_resolution}x7\")\n",
    "\n",
    "    return grad_schaefer, gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('brainsmash')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
